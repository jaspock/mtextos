
T4. Huggingface. Centralizaci√≥n de datasets y modelos
====================================

Contenidos

- Introducci√≥n
- Repositorio de Datasets
- Repositorio de Modelos pre-entrenados

## Introducci√≥n

Huggingface.co una compa√±√≠a centrada en el PLN la cual ha desarrollado las [**librer√≠as Transformers**](https://huggingface.co/transformers/), **centralizado datasets** y ha **creado modelos de aprendizaje pre-entrenados** disponibles a trav√©s de sus librer√≠as de programaci√≥n.
Las librer√≠as de Huggingface actualmente dan soporte a empresas muy importantes del mercado tecnol√≥gico. Ver <https://huggingface.co/>.

## Repositorio de Datasets

Proporciona conjuntos de datos para muchas tareas de PLN como clasificaci√≥n de texto, respuesta a preguntas, modelado de lenguaje, etc.  
Instalaci√≥n de librer√≠a de manipulaci√≥n de datasets
Para la **instalaci√≥n** de la librer√≠a de manipulaci√≥n de datasets se debe ejecutar la siguiente instrucci√≥n pip:

````
>>> pip install datasets
````

Para asegurarnos de que Transformers dataset se ha instalado correctamente es necesario ejecutar la siguiente instrucci√≥n:

````
>>> python -c "from datasets import load_dataset; print(load_dataset('squad', split='train')[0])"

````

Esta instrucci√≥n debe descargar la versi√≥n 1 del conjunto de datos de respuesta a preguntas de Stanford, cargar su divisi√≥n de entrenamiento e imprimir el primer ejemplo de entrenamiento de la siguiente manera:

````
{'id': '5733be284776f41900661182', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes"...', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'answers': {'text': array(['Saint Bernadette Soubirous'], dtype=object), 'answer_start': array([515], dtype=int32)}}
````

### Listar datasets disponibles en el repositorio

Para listar los conjuntos de datos disponibles es necesario ejecutar la siguiente funci√≥n ``datasets.list_datasets ()`` que pertenece a la clase ``datasets``.

````
>>> from datasets import list_datasets
>>> datasets_list = list_datasets()
>>> len(datasets_list)
656
>>> print(', '.join(dataset for dataset in datasets_list))
aeslc, ag_news, ai2_arc, allocine, anli, arcd, art, billsum, blended_skill_talk, blimp, blog_authorship_corpus, bookcorpus, boolq, break_data,
c4, cfq, civil_comments, cmrc2018, cnn_dailymail, coarse_discourse, com_qa, commonsense_qa, compguesswhat, coqa, cornell_movie_dialog, cos_e,
cosmos_qa, crime_and_punish, csv, definite_pronoun_resolution, discofuse, docred, drop, eli5, empathetic_dialogues, eraser_multi_rc, esnli,
event2Mind, fever, flores, fquad, gap, germeval_14, ghomasHudson/cqc, gigaword, glue, ‚Ä¶
````

Otra alternativa es:

1. Ir a la web <https://huggingface.co>
2. Seleccionar el men√∫ Datasets: <https://huggingface.co/datasets>
3. Filtrar por categor√≠a, idioma, tarea y/o licencia

### ¬øC√≥mo cargar datasets?

Haciendo uso de la funci√≥n ``load_dataset`` se nos permite recuperar cualquier dataset registrado en el repositorio. Por ejemplo, el dataset **MRPC** que ha sido proporcionado en el √≠ndice de referencia GLUE (<https://gluebenchmark.com/leaderboard>).

```
>>> from datasets import load_dataset
>>> dataset = load_dataset('glue', 'mrpc', split='train')
```

O podemos ver otro ejemplo como el de [**eHealth-KD**](https://knowledge-learning.github.io/ehealthkd-2020/)

````
>>> from datasets import load_dataset
>>> dataset = load_dataset("ehealth_kd")
````

No obstante, la librer√≠a ``datasets`` permite adem√°s **cargar conjuntos de datos propios** que no formen parte del repositorio. Por ejemplo:
````
>>> from datasets import load_dataset
>>> dataset = load_dataset('csv', data_files='my_file.csv')
````

Para m√°s detalles sobre las distintas funciones y par√°metros permitidos para manipular datasets ver la siguiente documentaci√≥n:
- <https://huggingface.co/docs/datasets/quicktour.html>

### Categor√≠as, tareas e idiomas de datasets

**9 Categor√≠as:**

```{image} /images/bloque3/t4/hf_dataset_categoria.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 300px
:align: center
```

Figura 1. Categor√≠as filtro de datasets

**M√°s de 134 tareas y m√°s de 194 idiomas:**

```{image} /images/bloque3/t4/hf_dataset_tareas_idiomas.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 300px
:align: center
```

Figura 2. Tareas e idiomas filtro datasets

## Repositorio de Modelos pre-entrenados

La biblioteca de Transformers permite el **uso de modelos previamente entrenados** para tareas de Comprensi√≥n del lenguaje natural (NLU), i.e. como analizar el sentimiento de un texto, y Generaci√≥n del lenguaje natural (NLG), i.e. como completar un mensaje con texto nuevo o traducir a otro idioma.
A groso modo listamos los modelos que nos podemos encontrar
- **An√°lisis de sentimiento**: Conocer si un texto es positivo o negativo
- **Generaci√≥n de texto** (en ingl√©s): proporcionar un mensaje para el cual el modelo generar√° un texto.
- **Reconocimiento de entidades nombradas** (NER): Dado en una oraci√≥n de entrada se etiqueta cada palabra con la entidad que esta representa (persona, lugar, organizaci√≥n, etc.)
- **Respuesta a preguntas**: Teniendo en cuenta un modelo de un contexto determinado, dado un pregunta se obtiene una respuesta.
- **Relleno de texto con m√°scara**: Dado un texto con palabras enmascaradas (p. Ej., Reemplazado por [M√ÅSCARA]), completar los espacios en blanco.
- **Resumen**: Generaci√≥n de un resumen a partir de texto extenso.
- **Traducci√≥n**: Traducci√≥n de un texto a otro idioma.
- **Extracci√≥n de caracter√≠sticas**: Obtener una representaci√≥n tensorial del texto.
Tomado de https://huggingface.co/transformers/quicktour.html

**Listado de tareas tal y como las podemos encontrar en el repositorio:**

```{image} /images/bloque3/t4/hf_modelos_tareas.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 300px
:align: center
```

Figura 3. Tareas filtro modelos
 
**Idiomas para los que se han entrenado los modelos:**
 
```{image} /images/bloque3/t4/hf_modelos_idiomas.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 300px
:align: center
```
Figura 4. Idiomas filtro modelos

Una explicaci√≥n detallada sobre cada una de estas tareas y ejemplos de uso con Huggingface Transformer la podemos encontrar en el siguiente enlace: 
- <https://huggingface.co/transformers/task_summary.html>

Ejemplo de An√°lisis de Sentimientos con Huggingface Transformer:

````
>>> from transformers import pipeline
>>> classifier = pipeline('sentiment-analysis')
>>> classifier('We are very happy to show you the ü§ó Transformers library.')
[{'label': 'POSITIVE', 'score': 0.9997795224189758}]
````

Si os fij√°is hemos cargado el modelo pre-entrenado llamado ``sentiment-analysis`` para utilizarlo como clasificador. Este **modelo** se puede **reentrenar** a escenarios espec√≠ficos si queremos realizando un ajuste sobre un nuevo corpus. Para m√°s detalles ver la clase pr√°ctica [``bloque3_p3_SA-Transformers-Training-FineTuning``](https://jaspock.github.io/mtextos/bloque3_p3_SA-Transformers-Training-FineTuning.html)


### ¬øC√≥mo buscar y reutilizar modelos pre-entrenados en la plataforma?

A continuaci√≥n, se listan los pasos a seguir:
1. Dirigirse al repositorio <https://huggingface.co/>
2. Seleccionar el men√∫ ``models`` que nos llevar√° a <https://huggingface.co/models>
3. Filtrar el listado de modelos seg√∫n la tarea, idioma, librer√≠a (Pytorch o TensorFlow), dataset sobre el que fue entrenado, o licencia. Por ejemplo:  tarea ``Text Classification``; idioma ``es``.
4. Elegir un modelo de la lista. Por ejemplo: ``bert-base-multilingual-uncased-sentiment``
5. Obtendremos la documentaci√≥n necesaria para utilizar el modelo.

**Conociendo el nombre del modelo** a utilizar entonces podemos **hacer uso** de este a trav√©s de la librer√≠a **Transformer**. 
En la propia documentaci√≥n se aporta el **c√≥digo de ejemplo** para hacer uso del modelo y en algunos casos una [**interfaz para probarlo**](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment): 

````
from transformers import AutoTokenizer, AutoModelForSequenceClassification
tokenizer = AutoTokenizer.**from_pretrained**("nlptown/bert-base-multilingual-uncased-sentiment")
model = AutoModelForSequenceClassification.**from_pretrained**("nlptown/bert-base-multilingual-uncased-sentiment")
````

### Configuraciones de modelos trasnformers
Los **modelos pre-entrenados** que se brindan en el repositorio se **basan** en alguna de las **arquitecturas Transformers** descrita en la documentaci√≥n del repositorio (<https://huggingface.co/transformers/model_doc/>).
Si tomamos como referencia la arquitectura de modelo Transformer [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html#overview) podemos conocer c√≥mo **gestionar** los distintos **par√°metros**, [**configuraciones de red neuronal**](https://huggingface.co/transformers/model_doc/distilbert.html#distilbertconfig), [**tokenizador**](https://huggingface.co/transformers/model_doc/distilbert.html#distilberttokenizer) y **ejemplos** para cada tipo de tarea.

````
>>> from transformers import DistilBertTokenizer, DistilBertModel
>>> import torch

>>> tokenizer = DistilBertTokenizer.**from_pretrained**('distilbert-base-uncased')
>>> model = DistilBertModel.**from_pretrained**('distilbert-base-uncased')

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state

````

Es importante conocer que las **configuraciones** de modelos Transformer ya **cuentan** con **modelos base pre-entrenados**. En el caso de ``DistilBERT`` podemos encontrar ``distilbert-base-uncased``.


Bibliograf√≠a

[1] <https://huggingface.co/>

