
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Técnicas para la minería de textos &#8212; Minería de Textos</title>
    
  <link rel="stylesheet" href="_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/estilos.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Representaciones de palabras y oraciones" href="bloque2_embeddings.html" />
    <link rel="prev" title="Introducción a la minería de textos" href="bloque1.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/logo-master-ca.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Minería de Textos</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Materiales de Minería de Textos
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1.html">
   Introducción a la minería de textos
  </a>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="current reference internal" href="#">
   Técnicas para la minería de textos
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="bloque2_embeddings.html">
     Representaciones de palabras y oraciones
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bloque2_recurrent.html">
     Redes neuronales recurrentes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bloque2_transformer.html">
     La arquitectura transformer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bloque2_pretrained.html">
     Modelos preentrenados
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bloque2_scraping.html">
     El raspado web
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bloque2_practica.html">
     Práctica de representaciones de palabras
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3.html">
   Aplicaciones de la minería de textos
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="content.html">
   Content in Jupyter Book
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="markdown.html">
     Markdown Files
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="notebooks.html">
     Content with notebooks
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/bloque2.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#revision-historica">
   Revisión histórica
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#los-inicios-y-la-traduccion-automatica">
     Los inicios y la traducción automática
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#los-primeros-sistemas-conversacionales">
     Los primeros sistemas conversacionales
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tecnicas-simbolicas-y-estadisticas">
     Técnicas simbólicas y estadísticas
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#los-primeros-modelos-neuronales">
     Los primeros modelos neuronales
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#el-entrenamiento-no-supervisado">
     El entrenamiento no supervisado
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aprendizaje-multitarea">
     Aprendizaje multitarea
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-embeddings">
     Word embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#el-inicio-del-auge-de-las-redes-neuronales">
     El inicio del auge de las redes neuronales
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modelos-recurrentes-sequence-to-sequence">
     Modelos recurrentes sequence-to-sequence
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#el-mecanismo-de-atencion">
     El mecanismo de atención
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#redes-neuronales-con-memoria-explicita">
     Redes neuronales con memoria explícita
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#la-arquitectura-transformer">
     La arquitectura transformer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modelos-preentrenados">
     Modelos preentrenados
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#el-modelo-gpt-3">
     El modelo GPT-3
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#corpus-disponibles">
     Corpus disponibles
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modelos-de-subpalabras">
     Modelos de subpalabras
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aplicaciones">
     Aplicaciones
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="tecnicas-para-la-mineria-de-textos">
<span id="label-tecnicas"></span><h1>Técnicas para la minería de textos<a class="headerlink" href="#tecnicas-para-la-mineria-de-textos" title="Permalink to this headline">¶</a></h1>
<p><em>«If I have seen further it is by standing on the shoulders of Giants.», Isaac Newton, 1675.</em></p>
<p>En este bloque se aborda el estudio de algunos modelos computacionales utilizados para procesar textos. El profesor de este bloque es Juan Antonio Pérez Ortiz.</p>
<div class="section" id="revision-historica">
<h2>Revisión histórica<a class="headerlink" href="#revision-historica" title="Permalink to this headline">¶</a></h2>
<p>A continuación se presenta una revisión histórica de la evolución del procesamiento del lenguaje natural desde sus inicios hasta nuestro días que sirve para hilvanar los contenidos de la asignatura. En los siguientes apartados de este bloque se entrará en detalles técnicos sobre el funcionamiento de los modelos neuronales, que son los que actualmente proporcionan los mejores resultados en las principales aplicaciones.</p>
<div class="note admonition">
<p class="admonition-title">Nota</p>
<p>Hacia final de curso deberías ser capaz de leer esta revisión histórica entendiendo cada aspecto explicado.</p>
</div>
<div class="section" id="los-inicios-y-la-traduccion-automatica">
<h3>Los inicios y la traducción automática<a class="headerlink" href="#los-inicios-y-la-traduccion-automatica" title="Permalink to this headline">¶</a></h3>
<p>El procesamiento del lenguaje natural ha tenido en la tarea de la traducción automática uno de sus principales catalizadores, por lo que la evolución de esta tarea concreta es un buen reflejo de la evolución de la disciplina entera. En el año 1947, el matemático e informático Warren Weaver planteaba en una carta a un colega la posibilidad de usar los computadores que comenzaban a aparecer en la época para traducir la documentación de la Unesco: «Recognizing fully, even though necessarily vaguely, the semantic difficulties because of multiple meanings, etc., I have wondered if it were unthinkable to design a computer which would translate.» Cinco años más tarde se celebraba una reunión de expertos en traducción automática en el MIT de los EUA, y en 1954 se presentó un sistema desarrollado conjuntamente por la Universidad de Georgetown e IBM que traducía unas 60 frases del ruso al inglés utilizando un pequeño diccionario y seis reglas de reordenamiento y selección léxica. Pese a la euforia inicial, la mejora en la calidad de estos sistemas fue tan lenta que en 1966 un comité de expertos recomendó al gobierno de los EUA recortar el presupuesto de investigación invertido en ellos.</p>
</div>
<div class="section" id="los-primeros-sistemas-conversacionales">
<h3>Los primeros sistemas conversacionales<a class="headerlink" href="#los-primeros-sistemas-conversacionales" title="Permalink to this headline">¶</a></h3>
<p>Una de las grandes pretensiones de la humanidad ha sido desde hace siglos la de disponer de ingenios parlantes. Ramón Llull en el siglo XII concibió un artilugio mecánico con el que pretendía que se pudiera probar o refutar cualquier proposición. En el capítulo 63 del segundo libro del Quijote, el Caballero de la Triste Figura queda sorprendido al entablar diálogo con una cabeza parlante, de la que nunca supo que se trataba de un engaño. Pero realmente los primeros sistemas conversacionales (<em>bots</em>) no aparecen hasta mediados de los 60, siendo ELIZA, que incluso podía adoptar diferentes personalidades o roles, o SHRDLU, que permitía interactuar con un mundo de bloques en lenguaje natural, los ejemplos más conocidos. Para construir un sistema de este tipo es necesario desarrollar técnicas de procesamiento del lenguaje natural y estas técnicas han tenido siempre un lugar privilegiado dentro del campo de la inteligencia artificial, hasta el punto de que cuando Alan Turing planteó en los años 40 su famosa prueba (<em>Turing’s test</em>) consideró la capacidad de diálogo como la demostración última de que una máquina pudiera pensar.</p>
</div>
<div class="section" id="tecnicas-simbolicas-y-estadisticas">
<h3>Técnicas simbólicas y estadísticas<a class="headerlink" href="#tecnicas-simbolicas-y-estadisticas" title="Permalink to this headline">¶</a></h3>
<p>Los años comprendidos entre la década de los 50 y la década de los 80 vieron toda una serie de avances en el campo, la mayoría de ellos dentro del terreno simbólico con sistemas que integraban reglas escritas por lingüistas. Las teorías acerca del lenguaje humano introducidas por Chomsky en los años 60 tuvieron una gran influencia en el desarrollo de estos formalismos. Sin embargo, hacia finales de la década de los 80 comienzan a aparecer sistemas competitivos basados en estadística y aprendizaje automático. En 1988 el investigador Frederik Jelinek enuncia su famosa frase, luego <a class="reference external" href="http://www.lrec-conf.org/lrec2004/doc/jelinek.pdf">matizada</a>, en relación a su sistema de reconocimiento de voz: «Every time I fire a linguist, my performance goes up». Además de contextualizar su afirmación, el mismo Jelinek reconoció posteriormente que las técnicas estadísticas pueden aplicarse en contextos híbridos en los que también tenga cabida el conocimiento lingüístico.</p>
<p>Las técnicas estadísticas (árboles de decisión, modelos ocultos de Markov, etc.) basadas en la explotación de grandes corpus de texto se ven propiciadas por la llegada de la web, que incrementa notablemente la cantidad de datos disponibles, y por el incremento en la capacidad de cómputo de los ordenadores. Pese a la predominancia de enfoques neuronales, las técnicas basadas en reglas o las estadísticas aún se usan hoy día en numerosos escenarios, como, por ejemplo, con lenguas con muy pocos recursos o para el pre o postprocesamiento de los textos usados en sistemas neuronales.</p>
</div>
<div class="section" id="los-primeros-modelos-neuronales">
<h3>Los primeros modelos neuronales<a class="headerlink" href="#los-primeros-modelos-neuronales" title="Permalink to this headline">¶</a></h3>
<p>Los primeros modelos neuronales que procesaban el lenguaje humano en la década de los 90 se centraron en tareas relativamente sencillas como predecir el siguiente elemento de una secuencia (por ejemplo, el siguiente carácter dado el prefijo de una frase). Aunque en la década de los 90 ya aparecieron trabajos que usaban redes neuronales con lenguaje natural, no es hasta unos años después cuando empiezan a obtenerse resultados significativos. Un artículo de Bengio presentó un modelo neuronal que aprende a la vez representaciones distribuidas de las palabras de la entrada (lo que luego se conocería como <em>word embedding</em>) y un modelo probabilístico de la siguiente palabra a la salida. Este <a class="reference external" href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">trabajo</a> de 2003 ya tiene tamaños de vocabulario aceptables (probablemente inviables unos años antes) de en torno a las 16000 palabras. El entrenamiento de este modelo con millones de parámetros llevó varias semanas en un sistema con 40 CPUs y la perplejidad total del modelo resultante demostró ser más baja que la de modelos estadísticos consolidados tal como los basados en trigramas suavizados.</p>
<p>Ya en aquel trabajo se plantea que “polysemous words are probably not well served by the model presented here, which assigns to each word a single point in a continuous semantic space” y se anticipa la necesidad de que una palabra pueda tener más de una representación en base a su contexto. También se menciona la conveniencia de utilizar redes neuronales recurrentes en lugar de redes <em>feedforward</em>. Las arquitecturas recurrentes se habían usado intensamente en la década anterior para tareas sencillas. De hecho, las unidades LSTM, que sustituyen a las neuronas tradicionales en estos modelos para mitigar el problema del gradiente evanescente, se propusieron en el año 1997 por Hochreiter y Schmidhuber.</p>
</div>
<div class="section" id="el-entrenamiento-no-supervisado">
<h3>El entrenamiento no supervisado<a class="headerlink" href="#el-entrenamiento-no-supervisado" title="Permalink to this headline">¶</a></h3>
<p>El tipo de entrenamiento del trabajo de Bengio se conoce como no supervisado, en el sentido de que no se necesitan etiquetar datos o asignar categorías durante el entrenamiento. Este tipo de aprendizaje fue defendido por Yann LeCun <a class="reference external" href="https://ruder.io/highlights-nips-2016/#generalartificialintelligence">en 2016</a> como una tarea muy importante en el camino hacia una inteligencia artificial de propósito general que pueda trabajar con el “sentido común”, que él define como la capacidad de predecir el pasado, presente o futuro de cualquier información disponible.</p>
<p>El entrenamiento no supervisado es una piedra angular en desarrollos como los word embeddings, los modelos preetrenados o los modelos de traducción de secuencias (por ejemplo, los usados en sistemas de traducción automática que no requieren corpus paralelo para su entrenamiento al explotar el concepto de <em>back-translation</em>).</p>
</div>
<div class="section" id="aprendizaje-multitarea">
<h3>Aprendizaje multitarea<a class="headerlink" href="#aprendizaje-multitarea" title="Permalink to this headline">¶</a></h3>
<p>Otro elemento fundamental en los modelos de procesamiento del lenguaje natural es el de aprendizaje multitarea (<em>multi-task learning</em>) que ya propuso Caruana en la década de los 90. En el contexto actual de las redes neuronales, esto significa que parte o la totalidad de los parámetros es compartida entre redes que resuelven problemas diferentes, pero relacionados: como caso límite, una red puede entrenarse para clasificar textos entre diferentes idiomas o para búsqueda de respuestas, resumen automático y traducción automática a la vez de manera que se aprovechen los efectos sinérgicos de combinbar todas las tareas.</p>
</div>
<div class="section" id="word-embeddings">
<h3>Word embeddings<a class="headerlink" href="#word-embeddings" title="Permalink to this headline">¶</a></h3>
<p>Aunque las representaciones distribuidas de palabras ya existían desde varios años antes, en 2013 Mikolov et al. muestran un método eficiente para su cálculo que dispara su uso. Los embeddings pueden calcularse mediante modelos complejos con grandes corpus de texto y luego emplearse en otras tareas. Existen colecciones como <a class="reference external" href="https://fasttext.cc/">fastText</a>, que incluyen embeddings para dos millones de palabras en inglés (obtenidos procesando un corpus de 16.000 millones de palabras) o embeddings multilingües para más de cien idiomas. También cabe aprender los embeddings para una tarea concreta, lo que consiste en aprender los valores de la tabla de <em>embeddings</em> como cualquier otro parámetro de la red (usualmente mediante descenso por gradiente estocástico). Los embeddings suelen tener propiedades geométricas interesantes (<em>Londres es a Reino Unido como París es a Francia</em>), pero también reflejan los sesgos de los textos en los que son entrenados (el embedding de <em>nurse</em> suele estar cerca del de <em>woman</em> y el <em>doctor</em> cerca de <em>man</em>).</p>
</div>
<div class="section" id="el-inicio-del-auge-de-las-redes-neuronales">
<h3>El inicio del auge de las redes neuronales<a class="headerlink" href="#el-inicio-del-auge-de-las-redes-neuronales" title="Permalink to this headline">¶</a></h3>
<p>A partir de 2013 aproximadamente los modelos neuronales empiezan a ser considerados como una opción seria para ciertas tareas de procesamiento del lenguaje natural, pero las técnicas tradicionales todavía tienen mayor peso. En esos momentos, en el terreno del procesamiento de imagen, los modelos neuronales ya superan competición tras competición a los más tradicionales. Las redes neuronales convolucionales DanNet en 2011 y AlexNet en 2013, por poner dos ejemplos, superaron en varios puntos a modelos hasta entonces consolidados y obtuvieron resultados <em>superhumanos</em>  en múltiples tareas de procesamiento de imágenes. Un equivalente en el área del procesamiento del lenguaje natural no llegaría hasta aproximadamente 2018 con la aparición de ELMo y BERT.</p>
<p>Una de las principales ventajas de los modelos neuronales respecto a los estadísticos es que se elimina en la mayor parte de los casos la necesidad de explicitar, diseñar y extraer las características relevantes de los textos en base al problema a resolver. La extracción de características (<em>feature engineering</em>) implica el conocimiento de expertos. Las redes neuronales profundas, sin embargo, son capaces de trabajar con los textos en bruto sin apenas ningún preprocesamiento previo.</p>
</div>
<div class="section" id="modelos-recurrentes-sequence-to-sequence">
<h3>Modelos recurrentes sequence-to-sequence<a class="headerlink" href="#modelos-recurrentes-sequence-to-sequence" title="Permalink to this headline">¶</a></h3>
<p>En 2014, Sutskever et al. presentan el modelo neuronal <em>sequence-to-sequence</em> que incluye dos redes neuronales (de una o más capas cada una; a más capas, más profunda se considera la representación aprendida) en un modelo conocido como codificador-descodificador (<em>encoder-decoder</em>), que es capaz de obtener una representación intermedia de la secuencia completa de entrada y “desenrollarla” a continuación en una nueva secuencia  de longitud no necesariamente igual a la de la de entrada. Aunque hay muchas tareas que se pueden especificar de esta forma (por ejemplo, el resumen de textos o la descripción textual automática de imágenes), el sistema es aplicado inicialmente a la traducción automática. En ciertos casos, el modelo supera la calidad de la traducción de los sistemas estadísticos (basados en encontrar las traducciones más probables para segmentos de varias palabras y combinarlas y reordenarlas teniendo en cuenta un modelo de lengua), que eran los dominantes hasta ese momento.</p>
</div>
<div class="section" id="el-mecanismo-de-atencion">
<h3>El mecanismo de atención<a class="headerlink" href="#el-mecanismo-de-atencion" title="Permalink to this headline">¶</a></h3>
<p>Si bien el modelo <em>sequence-to-sequence</em> supuso un gran avance, su principal limitación es que comprime toda la secuencia de entrada en un único vector a partir del cuál se va generando toda la secuencia de salida. En el año siguiente, Bahdanau et al. perfeccionan el modelo introduciendo el mecanismo de <em>atención</em>: en lugar de obtener un vector único para la frase de entrada combinando los vectores de cada palabra, el descodificador es capaz de utilizar las representaciones individuales de todas las palabras de la secuencia de entrada; cuando el descodificador va a generar la predicción de la siguiente palabra de la secuencia de salida, decide el grado o porcentaje de influencia de cada representación de la entrada y de las representaciones de la salida generadas hasta ese momento. La atención supuso otro salto cuantitativo en el rendimiento de los modelos que lleva a los grandes proveedores de sistemas de traducción automática a migrar rápidamente en los meses siguientes a tecnologías neuronales.</p>
</div>
<div class="section" id="redes-neuronales-con-memoria-explicita">
<h3>Redes neuronales con memoria explícita<a class="headerlink" href="#redes-neuronales-con-memoria-explicita" title="Permalink to this headline">¶</a></h3>
<p>A partir de 2014 comienzan a aparecer modelos neuronales avanzados (aunque los modelos simples datan de unas décadas antes) que integran de forma explícita una memoria de lectura/escritura en la que la red puede decidir almacenar ciertos vectores para su uso posterior. Los <em>computadores diferenciables neuronales</em> son ejemplo de este tipo de sistemas y son capaces de aprender algoritmos sencillos (ordenación o enrutamiento, por ejemplo) o trabajar en tareas en las que cierta información debe almacenarse durante largos periodos de tiempo.</p>
</div>
<div class="section" id="la-arquitectura-transformer">
<h3>La arquitectura transformer<a class="headerlink" href="#la-arquitectura-transformer" title="Permalink to this headline">¶</a></h3>
<p>El mecanismo de atención no solo se puede aplicar a la situación de un modelo recurrente en el que el descodificador tiene que integrar con diferentes grados de atención la información aportada por el codificador. En 2017, de nuevo con un enfoque inicial en la traducción automática, aparece la arquitectura conocida como <em>transformer</em>, que elimina la necesidad de utilizar redes recurrentes (que arrastran el problema del gradiente evanescente que dificulta la detección de dependencias a largo plazo, como cuando un adjetivo al final de una frase larga concuerda con un sustantivo que aparece al principio de esta, y que, además, son difíciles de paralelizar ya que cada paso depende del anterior) al aplicar el concepto de <em>autoatención</em>: la representación de cada palabra en el codificador se obtiene integrando mediante mecanismos de atención las representaciones en la capa anterior de todas las palabras de la frase de entrada; del mismo modo, el descodificador usa la atención para determinar la influencia de las representaciones de la entrada y del prefijo de la salida generado hasta el momento en la representación de la palabra a generar y, por ende, en las probabilidades emitidas para la siguiente palabra de la salida.</p>
</div>
<div class="section" id="modelos-preentrenados">
<h3>Modelos preentrenados<a class="headerlink" href="#modelos-preentrenados" title="Permalink to this headline">¶</a></h3>
<p>Aunque ya habían sido propuestos con anterioridad, en el año 2018 comienza la <em>revolución</em> de los llamados <em>modelos preentrenados</em>. Las representaciones vectoriales obtenidas por un codificador en sus diferentes capas pueden considerarse como <em>embeddings</em> contextuales de cada palabra. Si estos embeddings son representativos (es decir, sin son entrenados con suficientes cantidades de texto) pueden usarse para codificar la frase de entrada en redes neuronales que resuelvan tareas como detección de sentimiento, búsqueda de respuestas o inferencia en lenguaje natural. Pero los embeddings aprendidos por el codificador de un sistema de traducción automática tienen un par de inconvenientes:</p>
<ol class="simple">
<li><p>están condicionados por la lengua meta, ya que se entrenan para generar buenas traducciones por lo que no son monolingües; por ejemplo, el codificador de español aprendido para un traductor español-inglés puede representar con embeddings similares la palabra <em>canal</em> de una oración que habla sobre televisión y la palabra <em>canal</em> en una oración que habla sobre cauces de agua, simplemente porque en ambos casos la traducción es <em>channel</em>;</p></li>
<li><p>para obtener estos embeddings es necesario entrenar un sistema de traducción automática con grandes cantidades de corpus bilingües y hay muchas lenguas para las que no existe este tipo de información supervisada en cantidad suficiente.</p></li>
</ol>
<p>Por ello, los sistemas preentrenados se obtienen mediante tareas no supervisadas. Los primeros de estos modelos como ELMo entrenaban el sistema para que predijera la siguiente palabra, pero los sistemas posteriores como BERT plantean una tarea no supervisada más difícil que permite obtener representaciones más complejas y elaboradas; esta tarea, conocida como <em>mask filling</em> consiste en sustituir aleatoriamente algunas palabras de la entrada por una marca especial (<em>mask</em>) y enseñar a la red a maximizar la probabilidad de las palabras originales a la salida. Existen múltiples variaciones de esta estrategia no supervisada de entrenamiento, pero su planteamiento básico es similar a este.</p>
<p>El uso de un modelo preentrenado en una tarea concreta de procesamiento del lenguaje natural pasa por añadir una o más capas a la salida del modelo preentrenado y entrenar la salida del modelo ampliado para la tarea en cuestión, lo que se denomina <em>ajuste fino</em> (<em>fine-tuning</em>). Los pesos de la parte correspondiente al modelo preetrenado suelen dejarse congelados, pero también podrían ajustarse o incorporar entre las capas los llamados <em>adaptadores</em>. La arquitectura de un modelo preentrenado (por ejemplo, BERT) es como la del codificador de un transformer cuando el modelo resultante se va a usar para tareas de clasificación de secuencias; la arquitectura es como la de un transformer completo (por ejemplo, BART o T5, o sus versiones multilingües mBART o mT5) cuando el modelo resultante se va a usar para tareas que generan secuencias a partir de secuencias. Hay también modelos preentrenados para tareas específicas: así, para la continuación de secuencias la arquitectura de GPT-3 usa el descodificador de un transformer; y M2M es un transformer entrenado sobre decenas de corpus bilingües de múltiples pares de idiomas.</p>
</div>
<div class="section" id="el-modelo-gpt-3">
<h3>El modelo GPT-3<a class="headerlink" href="#el-modelo-gpt-3" title="Permalink to this headline">¶</a></h3>
<p>Apartado sin terminar.</p>
</div>
<div class="section" id="corpus-disponibles">
<h3>Corpus disponibles<a class="headerlink" href="#corpus-disponibles" title="Permalink to this headline">¶</a></h3>
<p>Apartado sin terminar.</p>
</div>
<div class="section" id="modelos-de-subpalabras">
<h3>Modelos de subpalabras<a class="headerlink" href="#modelos-de-subpalabras" title="Permalink to this headline">¶</a></h3>
<p>Un cuello de botella de los modelos neuronales es el elevado tamaño de la matriz de embeddings o el cálculo de la función softmax en la capa de salida. Ambos están en función del tamaño del vocabulario. Para aligerarlo, se propuso el uso de unidades más pequeñas que la palabra. Por ejemplo, si en lugar de tener un embedding para cada forma del presente de indicativo de los <em>cantar</em>, <em>bailar</em>, <em>danzar</em> y <em>pintar</em>, usamos representaciones para <em>cant</em>, <em>bail</em>, <em>danz</em> <em>pint</em>, <em>o</em>, <em>as</em>, <em>a</em>, <em>amos</em> <em>áis</em> y <em>an</em> habremos reducido el tamaño del vocabulario de 24 a 10. Estos tokens que representan unidades conocidas como subpalabras se obtienen mediante sencillas técnicas estadísticas de conteo como BPE o SentencePiece.</p>
</div>
<div class="section" id="aplicaciones">
<h3>Aplicaciones<a class="headerlink" href="#aplicaciones" title="Permalink to this headline">¶</a></h3>
<p>Algunas tareas de bajo nivel típicas del procesamiento del lenguaje natural son la lematización, la segmentación morfológica, el etiquetado de partes de la oración, la inducción de gramáticas, el análisis sintáctico, la separación de un texto en oraciones, el reconocimiento de entidades nombradas, la extracción terminológica, la desambiguación del sentido de las palabras, la extracción de relaciones, el análisis semántico o la resolución de coreferencias. Aplicaciones de más alto nivel incluyen la obtención automática de resúmenes, el análisis de sentimiento u opinión, la generación de textos, los agentes conversacionales, los correctores ortográficos o de estilo, la traducción automática, la búsqueda de respuestas o la modelización del sentido común, entre otros.</p>
<div class="toctree-wrapper compound">
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="bloque1.html" title="previous page">Introducción a la minería de textos</a>
    <a class='right-next' id="next-link" href="bloque2_embeddings.html" title="next page">Representaciones de palabras y oraciones</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Universitat d'Alacant<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>